---
title: Spotify Hits
author: Samuel Louissaint
date: '2021-04-20'
slug: spotify-hits
categories: []
tags: []
subtitle: ''
summary: ''
authors: []
lastmod: '2021-04-20T12:47:13-04:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
---

It doesn't take an avid music listener to notice that many popular songs have the same formula to them. Radio stations can often sound like the same song is playing on repeat, with slightly different variants of the same song gaining recognition and winning awards.

This very thought led me to find a Spotify dataset that contained all Billboard top 10 songs from the years 2010 to 2019. All there was left to do was explore it a bit.
```{r include=FALSE}
library(tidyverse)
library(readr)
library(gt)
library(caret)
library(gam)
library(MLmetrics)
library(grid)
library(gridExtra)
topten = read.csv("~/Data Science/Personal Website/saml/content/post/2021-04-20-spotify-hits/top10s.csv")
no1 = read.csv("~/Data Science/Personal Website/saml/content/post/2021-04-20-spotify-hits/Number1s.csv")
head(topten)
head(no1)
```

I started by doing some exploratory analysis of the dataset.
While the categorical variables are self-explanatory, here's a key for the quantitative variables for reference:

    - acousticness (Ranges from 0 to 1)
    - danceability (Ranges from 0 to 1)
    - energy (Ranges from 0 to 1)
    - duration_ms (Integer typically ranging from 200k to 300k)
    - instrumentalness (Ranges from 0 to 1)
    - valence (Ranges from 0 to 1)
    - popularity (Ranges from 0 to 100)
    - tempo (Float typically ranging from 50 to 150)
    - liveness (Ranges from 0 to 1)
    - loudness (Float typically ranging from -60 to 0)
    - speechiness (Ranges from 0 to 1)

```{r echo=TRUE}
summary(topten)
```

Songs that generally have the same sound could be explained by a number of reasons, such as coming from the same genre or artist. With that in mind, I wanted to explore the relationship between top songs and the artist making them.
```{r echo=FALSE}
Data1a = topten %>%
  transmute(artist, year) %>%
  group_by(artist)
Data1b = Data1a %>%
  count(artist) %>%
  filter(n >= 10)
ggplot(Data1b)+
  geom_point(mapping = aes(x = artist, y = n), color = 'green') +
  ggtitle("# of Top 10 Appearances by Top Artists (n >= 10)") +
  ylab("# of Songs") +
  xlab("Artist") +
  theme(axis.text.x = element_text(face="bold",size= 10, angle= 30), plot.margin= unit(c(0,1,0,0),"cm"))
```

```{r echo=FALSE}
ArtistGenres <- topten %>%
  group_by(year, top.genre) %>%
  count(artist)%>%
  ungroup() %>%
  spread(key = top.genre, value = n) %>%
  mutate(total = rowSums(select(., !c("year", "artist"))))

ArtistGenres$total <- rowSums(select(ArtistGenres, !all_of(c("year", "artist"))), na.rm = TRUE)

ArtistGenres$genres <- rowSums(!is.na(select(ArtistGenres, !all_of(c("year", "artist", "total")))))

ByYear <- ArtistGenres %>%
  select(all_of(c("year", "genres", "total", "artist"))) %>%
  filter(total >1) %>%
  group_by(year, genres) %>%
  count() %>%
  ungroup()

tableOut <- ByYear %>%
  gt() %>%
  tab_header(title = "# of Genres for Artists with Multiple Hits", subtitle = "For each artitst with multiple hits in a year, in  how many genres were their  hits?") %>%
  cols_label(year = "Year", genres = "# of Genres", n = "# of Artists") %>%
  cols_align("center")

tableOut
```
This brief investigation tells us the top artists over the 10-year period as well as the fact that their respective hits were all within the same genre. This implies that artists are hesitant to change the formulas that bring them success. Or that listeners aren't interested in the songs that do change that formula. 


I then started wondering about one-hit wonders and the effect that the addition of new streaming platforms had on the variety of artists that created hits.
```{r echo=FALSE}
SongsYr <- topten %>%
  group_by(year) %>%
  count(artist)%>%
  ungroup()

Artists2Hits <- SongsYr %>%
  filter(n>1) %>%
  count(year) %>%
  mutate(artists = n, .keep = "unused")

ggplot(data = Artists2Hits) + 
  geom_bar(mapping = aes(x = year, y = artists), fill = 'green', stat = "identity") +
  ggtitle("# of artists with more than one hit song by year") +
  scale_x_continuous("Year", labels = seq(2010, 2020), breaks = seq(2010, 2020)) + 
  ylab ("Artists with multiple hits") + theme_minimal()
```
It's hard to find a trend in the first half of the decade. However, after the spike in 2015, there's a marked decrease in the number of artists that were able to produce multiple hit songs within a year. Rather than attributing this trend solely to streaming platforms, it may also be a byproduct of increasing ways to produce music.

Me, Myself, and I by G-Eazy was unmistakably one of the biggest hits on the radio when it was released. Though what I found most interesting was the number of songs that went by the same title though created by different artists (Big Sean and Beyonce to name a couple) in different genres. This led me to investigate whether the subject of a song, expressed by its title, had an effect on whether it would be a hit.

```{r echo=FALSE}
youtitle = topten %>%
  filter(str_detect(title,"You")) %>%
  count(title)
You= sum(youtitle$n)-7
#7 of these 82 don't fall under the you/your/you're family, and will be excluded. Leaving 75.
lovetitle = topten %>%
  filter(str_detect(title,"Love")) %>%
  count(title)
Love = sum(lovetitle$n)
metitle = topten %>%
  filter(str_detect(title,"Me")) %>%
  count(title)
Me = sum(metitle$n)-10
#10 of these 54 either have the letters "me" as part of a larger word, or the "me" isn't part of the song title (i.e. being from a movie soundtrack with "me" in its title). Leaving 44.
ititle = topten %>%
  filter(str_detect(title,"I" )) %>%
  count(title)
I = sum(ititle$n)-47
#47 of the 84 simply have the capital letter "I" in them. Leaving 37.
ittitle = topten %>%
  filter(str_detect(title,"It" )) %>%
  count(title)
It = sum(ittitle$n)
mytitle = topten %>%
  filter(str_detect(title,"My" )) %>%
  count(title)
My= sum(mytitle$n)
q2data = data.frame("Word" = c("You","Love","Me","I","It","My"), "Count" = c(You,Love,Me,I,It,My))
ggplot(data=q2data) +
  geom_point(aes(x=Word, y=Count, color=Word,size=.5)) +
  ylab("# of Songs in Billboard Top 10") +
  ggtitle("Relationship between Title Subject and Song Success")
```

I did realize there is a level of overlap, such as the possibility that a song title has multiple subjects and even a verb, allowing a song titled "I Love You" to be cross-listed 3 times. Despite this, "Love" had particularly interesting results in my opinion. Being the only word that is both a noun and a verb, I expected it to appear more often than all except for "You" which was understandably common.

Finally, I'm sure you've noticed the variable "pop" is intuitively related to the likelihood that a song would be in the Billboards Top 10. So I ventured to identify the relationship between the other quantitative variables and a song being a Billboards Number 1. For this, I created a dummy variable that represented whether a song was ever a Billboards Number 1. 
```{r echo = FALSE}
q1top10 = topten %>%
  select(2:15)
no1 = no1 %>%
  mutate(No.1 = 1) %>%
  select(-(2:14))
no1s = q1top10 %>%
  left_join(no1, by="title","artist")
no1s[is.na(no1s)] = 0
no1s = no1s %>%
  select(5:15, -pop) %>%
  mutate(No.1 = as.factor(ifelse(No.1 == 0, 'no', 'yes')))
set.seed(30)
index <- createDataPartition(no1s$No.1, p = 0.8, list = F)
dtrain <- no1s[index,]
dtest <- no1s[-index,]
```

Unfortunately the target data is very unbalanced, with only 72 of 603 observations having been a billboards number 1. In an attempt to build a substantive model, I tried both oversampling and undersampling. But before that, I created a model using random forest with the unbalanced data to demonstrate the issue.
```{r, warning=FALSE, echo=FALSE}
set.seed(30)
tcontrol <- trainControl(method = 'repeatedcv', number = 10, repeats = 10, verboseIter = F)

rf <- train(No.1~., data = dtrain, method = 'rf', preProcess = c('scale', 'center'), trControl = tcontrol)
final <- data.frame(actual = dtest$No.1, predicted = predict(rf, newdata = dtest, type = 'raw'))

rf_cm <- confusionMatrix(final$predicted, dtest$No.1)
rf_cm
```
Based on the "No Information Rate" it's clear that there's such a strong majority in the target feature, the model simply predicts that practically no songs are  Billboards number 1s. This leads to a good accuracy score but awful kappa and specificity scores.  

Think of the kappa value as being indicative of how much of the prediction would be accurate even if all predictions were allocated to the majority class, 1 indicating it's a useful model despite unbalanced data and 0 meaning it isn't helpful.  

To address this, first I tried undersampling and arrived at the following output.
```{r, warning = FALSE,  echo = FALSE}
set.seed(30)
unders <- trainControl(method = 'repeatedcv', number = 10, repeats = 10, verboseIter = F, sampling = 'down')
rf_under <- train(No.1~., data = dtrain, method = 'rf', preProcess = c('scale', 'center'), trControl = unders)
final_under <- data.frame(actual = dtest$No.1, predicted = predict(rf_under, newdata = dtest))
rf_under_cm <- confusionMatrix(final_under$predicted, dtest$No.1)
rf_under_cm
```
While the undersampled model had a more even distribution of predictions for each class, this resulted in a test accuracy score that's worse than a theoretical coin flip.  
The random forest model with oversampling resulted in the following output.
```{r, warning = FALSE, echo = FALSE}
set.seed(30)
overs <- trainControl(method = 'repeatedcv', number = 10, repeats = 10, verboseIter = F, sampling = 'up')
rf_over <- train(No.1~., data = dtrain, method = 'rf', preProcess = c('scale', 'center'), trControl = overs)
final_over <- data.frame(actual = dtest$No.1, predicted = predict(rf_over, newdata = dtest))
rf_over_cm <- confusionMatrix(final_over$predicted, dtest$No.1)
rf_over_cm
```
The oversampled model ended up creating the same predictions as the original model, which again is a good accuracy score, but doesn't mean that the model is necessarily useful.  
The box and whisker plots below demonstrate the accuracy and kappa for repeated attempts at training each model. For all three, the median kappa values are very low, implying that the models themselves didn't improve prediction much beyond predicting only the majority class.
```{r, echo = FALSE}
models <- list(original =  rf,
               under = rf_under,
               over = rf_over)
resampling <- resamples(models)
bwplot(resampling)
```
I found one interesting insight when plotting the variable importance for each model. Variable importance in the Caret package ranges from 0-100. Notice in the plot below, variable importance is much higher in oversampled model than in the undersampled and original models. This suggests that the model *did* learn some sort of pattern for prediction. However, the predictions from that model didn't change from those in the original model.
```{r, echo = FALSE}
oplot <- plot(varImp(rf, scale = F), main = 'Original')
ovplot <- plot(varImp(rf_over, scale = F), main = 'Oversampling')
unplot <- plot(varImp(rf_under, scale = F), main = 'Undersampling')
grid.arrange(oplot, ovplot, unplot, nrow = 1, ncol = 3)
```
Based on this brief analysis, it appears that unaccounted for variables such as artist popularity, genre, and possibly the relationship between genre and year, do much of the heavy lifting for a song's success. And honestly no surprise there. This would explain why artists artists don't (or do) feel comfortable changing their styles.